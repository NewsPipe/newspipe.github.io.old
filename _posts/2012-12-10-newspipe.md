---
category: newspipe
title: 'newspipe'

layout: null
---
newspipe is a complete pipeline for collecting online newspaper article. The articles are stored in a MongoDB. The whole pipeline is dockerized, thus the user does not need to worry about dependencies. Additionally, docker-compose is available to increase the useability for the user.

## Requirement
- docker
- docker-compose
- a `.env` file with following arguments:
  ```MONGO_ROOT_USER=devroot
  MONGO_ROOT_PASSWORD=devroot
  MONGOEXPRESS_LOGIN=dev
  MONGOEXPRESS_PASSWORD=dev
  POSTGRES_USER=airflow
  POSTGRES_PASS=airflow
  ```
  
## Getting Started
To start newspipe, clone the repository and start the containers with docker-compose:
```git clone https://github.com/NewsPipe/newspipe.git
cd newspipe
docker-compose up
```
- [mongo-express](https://github.com/mongo-express/mongo-express) is running on `localhost:8081`. The MongoDB itself is available on port `27017`. 
- The airflow application is available on `localhost:8080`. You will see the airflow dashboard with the default examples. Default username and password is admin.

## Adding article sources
Each crawler is defined as DAG in 'dag'. To add a data source, you must therefore add DAGs in the `dags` folder. A DAG is a Python script that contains the settings for an entire crawling pipeline. Use the default example as a template. The DAGs are very simple and straightforward.

```
import os
import datetime
from dag_factory import create_dag
# url of newspaper source
url = "taz.de" 
# defining the crawling times
airflow_config = {'schedule_interval': '@hourly', # set a interval, for continuous crawling
                  'start_date': datetime.datetime(2020, 6, 4, 21), # set a date, on which the dag will run
                  'end_date':datetime.datetime(2020, 6, 5, 6), # optinal, set if it is needed
                  }
# create DAG
DAG = create_dag(url=url,
                 airflow_config=airflow_config,
                 name=os.path.basename(__file__))
```

## Starting a DAG
After defining your sources, open `localhost:8080` on your browser and turn on the scheduler with the trigger (located next to the DAG name). The DAGs will be automatically scheduled afterward
